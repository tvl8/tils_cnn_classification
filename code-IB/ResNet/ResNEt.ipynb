{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defdeeb2-17a4-4e8b-b1a6-0cd32261111d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 17:51:19.699638: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00145ba3-2c7c-4292-8e6a-f690d64b348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'ResNet'\n",
    "# Define the data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Set the paths for the datasets\n",
    "base_folder = \"/Users/ishaanbabbar/Desktop/GT Summer 2024/DSAN 6500/Project/wsirois\"\n",
    "train_folder = os.path.join(base_folder, \"train\")\n",
    "test_folder = os.path.join(base_folder, \"test\")\n",
    "validation_folder = os.path.join(base_folder, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c423e79d-3e2e-4122-ad75-bbc129b634f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImageFolder(root=train_folder, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_folder, transform=transform)\n",
    "validation_dataset = ImageFolder(root=validation_folder, transform=transform)\n",
    "\n",
    "batchsize = 32\n",
    "numworkers = 4\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=numworkers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False, num_workers=numworkers)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batchsize, shuffle=False, num_workers=numworkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7d69027-de4c-487e-971c-653b950e6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def train_one_epoch(model, data_loader, criterion, optimizer, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    top5_correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Update the tqdm description to show current epoch\n",
    "    for inputs, labels in tqdm(data_loader, desc=f\"Epoch {epoch}/{num_epochs} - Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "         # Add top 5 accuracy\n",
    "        _, top5_predicted = outputs.topk(5,1, True, True)\n",
    "        top5_correct += top5_predicted.eq(labels.view(-1, 1).expand_as(top5_predicted)).sum().item()\n",
    "        \n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    top5_accuracy = top5_correct / total_predictions\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    return avg_loss, accuracy, top5_accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3684f66-21ba-4d3f-951a-5349127ceaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device, epoch, num_epochs, phase='Validation'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    top5_correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Update the tqdm description to show current epoch and phase (Validation or Testing)\n",
    "    for inputs, labels in tqdm(data_loader, desc=f\"Epoch {epoch}/{num_epochs} - {phase}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "            \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Add top 5 accuracy\n",
    "        _, top5_predicted = outputs.topk(5,1, True, True)\n",
    "        top5_correct += top5_predicted.eq(labels.view(-1, 1).expand_as(top5_predicted)).sum().item()\n",
    "        \n",
    "        \n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "            \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    top5_accuracy = top5_correct / total_predictions\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    return avg_loss, accuracy, top5_accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8afa9e8-acf1-476c-9ebb-3ef3842138dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /Users/ishaanbabbar/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████████████████████████████████| 97.8M/97.8M [00:02<00:00, 39.2MB/s]\n",
      "Epoch 1/10 - Training: 100%|██████████████████████| 5/5 [00:57<00:00, 11.48s/it]\n",
      "Epoch 1/10 - Validation: 100%|████████████████████| 5/5 [00:40<00:00,  8.17s/it]\n",
      "Epoch 1/10 - Testing: 100%|███████████████████████| 5/5 [00:41<00:00,  8.35s/it]\n",
      "/var/folders/yp/ms18mh5d3gl1jr8k6cjqw0h40000gn/T/ipykernel_29048/1726919618.py:72: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
      "Epoch 2/10 - Training: 100%|██████████████████████| 5/5 [00:57<00:00, 11.42s/it]\n",
      "Epoch 2/10 - Validation: 100%|████████████████████| 5/5 [00:40<00:00,  8.12s/it]\n",
      "Epoch 2/10 - Testing: 100%|███████████████████████| 5/5 [00:41<00:00,  8.24s/it]\n",
      "Epoch 3/10 - Training: 100%|██████████████████████| 5/5 [00:55<00:00, 11.15s/it]\n",
      "Epoch 3/10 - Validation: 100%|████████████████████| 5/5 [00:40<00:00,  8.11s/it]\n",
      "Epoch 3/10 - Testing: 100%|███████████████████████| 5/5 [00:39<00:00,  7.88s/it]\n",
      "Epoch 4/10 - Training: 100%|██████████████████████| 5/5 [00:57<00:00, 11.49s/it]\n",
      "Epoch 4/10 - Validation: 100%|████████████████████| 5/5 [00:42<00:00,  8.52s/it]\n",
      "Epoch 4/10 - Testing: 100%|███████████████████████| 5/5 [00:41<00:00,  8.22s/it]\n",
      "Epoch 5/10 - Training: 100%|██████████████████████| 5/5 [00:58<00:00, 11.64s/it]\n",
      "Epoch 5/10 - Validation: 100%|████████████████████| 5/5 [00:43<00:00,  8.63s/it]\n",
      "Epoch 5/10 - Testing: 100%|███████████████████████| 5/5 [00:40<00:00,  8.13s/it]\n",
      "Epoch 6/10 - Training: 100%|██████████████████████| 5/5 [00:56<00:00, 11.24s/it]\n",
      "Epoch 6/10 - Validation: 100%|████████████████████| 5/5 [00:40<00:00,  8.04s/it]\n",
      "Epoch 6/10 - Testing: 100%|███████████████████████| 5/5 [00:41<00:00,  8.30s/it]\n",
      "Epoch 7/10 - Training: 100%|██████████████████████| 5/5 [00:54<00:00, 10.90s/it]\n",
      "Epoch 7/10 - Validation: 100%|████████████████████| 5/5 [00:40<00:00,  8.13s/it]\n",
      "Epoch 7/10 - Testing: 100%|███████████████████████| 5/5 [00:40<00:00,  8.07s/it]\n",
      "Epoch 8/10 - Training: 100%|██████████████████████| 5/5 [00:54<00:00, 10.90s/it]\n",
      "Epoch 8/10 - Validation: 100%|████████████████████| 5/5 [00:39<00:00,  7.95s/it]\n",
      "Epoch 8/10 - Testing: 100%|███████████████████████| 5/5 [00:40<00:00,  8.10s/it]\n",
      "Epoch 9/10 - Training: 100%|██████████████████████| 5/5 [00:53<00:00, 10.75s/it]\n",
      "Epoch 9/10 - Validation: 100%|████████████████████| 5/5 [00:40<00:00,  8.15s/it]\n",
      "Epoch 9/10 - Testing: 100%|███████████████████████| 5/5 [00:40<00:00,  8.08s/it]\n",
      "Epoch 10/10 - Training: 100%|█████████████████████| 5/5 [00:55<00:00, 11.13s/it]\n",
      "Epoch 10/10 - Validation: 100%|███████████████████| 5/5 [00:39<00:00,  7.99s/it]\n",
      "Epoch 10/10 - Testing: 100%|██████████████████████| 5/5 [00:41<00:00,  8.24s/it]\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"/Users/ishaanbabbar/Desktop/GT Summer 2024/DSAN 6500/Project/wsirois/output_ResNet\"\n",
    "import torchvision.models.detection as detection\n",
    "\n",
    "# Load and modify the pretrained SSD model\n",
    "model = models.resnet50(pretrained = True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
    "model.to(device)\n",
    "\n",
    "#num_classes = len(train_dataset.classes)\n",
    "#model.classifier = nn.Sequential(\n",
    "#    nn.Dropout(p=0.2, inplace=False),\n",
    "#    nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "#)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler - define your warmup_cosine_annealing scheduler here\n",
    "\n",
    "num_epochs = 10\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.1**(epoch // 30))\n",
    "\n",
    "writer = SummaryWriter('../runs/AlexNet_experiment')\n",
    "\n",
    "# Initialize DataFrame to store metrics\n",
    "columns = [\n",
    "    'Epoch', 'Training Loss', 'Validation Loss', 'Test Loss',\n",
    "    'Training Accuracy', 'Validation Accuracy', 'Test Accuracy',\n",
    "    'Training Precision', 'Training Recall', 'Training F1-Score',\n",
    "    'Validation Precision', 'Validation Recall', 'Validation F1-Score',\n",
    "    'Test Precision', 'Test Recall', 'Test F1-Score',\n",
    "    'Epoch Duration', 'Training Top-5 Accuracy', 'Validation Top-5 Accuracy', 'Test Top-5 Accuracy'\n",
    "]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "best_val_accuracy = 0  # Initialize best validation accuracy for checkpointing\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_accuracy, train_prec, train_rec, train_f1, train_top5_acc = train_one_epoch(\n",
    "    model, train_loader, criterion, optimizer, device, epoch, num_epochs)\n",
    "\n",
    "    val_loss, val_accuracy, val_prec, val_rec, val_f1, val_top5_acc = evaluate(\n",
    "    model, validation_loader, criterion, device, epoch, num_epochs, phase='Validation')\n",
    "\n",
    "    test_loss, test_accuracy, test_prec, test_rec, test_f1, test_top5_acc = evaluate(\n",
    "    model, test_loader, criterion, device, epoch, num_epochs, phase='Testing')\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    scheduler.step()\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalars('Loss', {'Train': train_loss, 'Validation': val_loss, 'Test': test_loss}, epoch)\n",
    "    writer.add_scalars('Accuracy', {'Train': train_accuracy, 'Validation': val_accuracy, 'Test': test_accuracy}, epoch)\n",
    "    writer.add_scalars('Precision', {'Train': train_prec, 'Validation': val_prec, 'Test': test_prec}, epoch)\n",
    "    writer.add_scalars('Recall', {'Train': train_rec, 'Validation': val_rec, 'Test': test_rec}, epoch)\n",
    "    writer.add_scalars('F1-Score', {'Train': train_f1, 'Validation': val_f1, 'Test': test_f1}, epoch)\n",
    "    writer.add_scalars('Top-5 Accuracy', {'Train': train_top5_acc, 'Validation': val_top5_acc, 'Test': test_top5_acc}, epoch)\n",
    "\n",
    "    # Update DataFrame\n",
    "    new_row = {\n",
    "        'Epoch': epoch + 1, 'Training Loss': train_loss, 'Validation Loss': val_loss, 'Test Loss': test_loss,\n",
    "        'Training Accuracy': train_accuracy, 'Validation Accuracy': val_accuracy, 'Test Accuracy': test_accuracy,\n",
    "        'Training Precision': train_prec, 'Training Recall': train_rec, 'Training F1-Score': train_f1,\n",
    "        'Validation Precision': val_prec, 'Validation Recall': val_rec, 'Validation F1-Score': val_f1,\n",
    "        'Test Precision': test_prec, 'Test Recall': test_rec, 'Test F1-Score': test_f1,\n",
    "        'Epoch Duration': epoch_duration, 'Training Top-5 Accuracy': train_top5_acc, 'Validation Top-5 Accuracy': val_top5_acc,\n",
    "        'Test Top-5 Accuracy': test_top5_acc\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    # Checkpointing\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(output_folder, f'AlexNet_best.pth'))\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = f\"AlexNet_training_results_{timestamp}.csv\"\n",
    "df.to_csv(os.path.join(output_folder, csv_filename), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
