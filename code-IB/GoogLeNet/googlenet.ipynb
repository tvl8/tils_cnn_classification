{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f413cf-1498-49d4-ad45-da6ea1aa530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 20:26:51.155967: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# setting device on GPU if available, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e12ac0-f1b3-42ba-b00e-2f0ea1371388",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'GoogleNet'\n",
    "# Define the data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Set the paths for the datasets\n",
    "base_folder = \"/Users/ishaanbabbar/Desktop/GT Summer 2024/DSAN 6500/Project/wsirois\"\n",
    "train_folder = os.path.join(base_folder, \"train\")\n",
    "test_folder = os.path.join(base_folder, \"test\")\n",
    "validation_folder = os.path.join(base_folder, \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f503aa5-0e57-4444-b5fd-6bbf282137c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = ImageFolder(root=train_folder, transform=transform)\n",
    "test_dataset = ImageFolder(root=test_folder, transform=transform)\n",
    "validation_dataset = ImageFolder(root=validation_folder, transform=transform)\n",
    "\n",
    "batchsize = 32\n",
    "numworkers = 4\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True, num_workers=numworkers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False, num_workers=numworkers)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batchsize, shuffle=False, num_workers=numworkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e68e07b-68b3-44d9-b0bd-0f6e756ea597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def train_one_epoch(model, data_loader, criterion, optimizer, device, epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    top5_correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Update the tqdm description to show current epoch\n",
    "    for inputs, labels in tqdm(data_loader, desc=f\"Epoch {epoch}/{num_epochs} - Training\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "         # Add top 5 accuracy\n",
    "        _, top5_predicted = outputs.topk(5,1, True, True)\n",
    "        top5_correct += top5_predicted.eq(labels.view(-1, 1).expand_as(top5_predicted)).sum().item()\n",
    "        \n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    top5_accuracy = top5_correct / total_predictions\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    return avg_loss, accuracy, top5_accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d046cfc-e9be-4f86-a7cf-1974fee18100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion, device, epoch, num_epochs, phase='Validation'):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    top5_correct = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    # Update the tqdm description to show current epoch and phase (Validation or Testing)\n",
    "    for inputs, labels in tqdm(data_loader, desc=f\"Epoch {epoch}/{num_epochs} - {phase}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "            \n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Add top 5 accuracy\n",
    "        _, top5_predicted = outputs.topk(5,1, True, True)\n",
    "        top5_correct += top5_predicted.eq(labels.view(-1, 1).expand_as(top5_predicted)).sum().item()\n",
    "        \n",
    "        \n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "            \n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    top5_accuracy = top5_correct / total_predictions\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='macro', zero_division=0)\n",
    "\n",
    "    return avg_loss, accuracy, top5_accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "281a098c-d675-489a-ae7e-523032840b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1/20 - Training: 100%|██████████████████████| 5/5 [00:39<00:00,  7.89s/it]\n",
      "Epoch 1/20 - Validation: 100%|████████████████████| 5/5 [00:35<00:00,  7.03s/it]\n",
      "Epoch 1/20 - Testing: 100%|███████████████████████| 5/5 [00:36<00:00,  7.29s/it]\n",
      "/var/folders/yp/ms18mh5d3gl1jr8k6cjqw0h40000gn/T/ipykernel_12776/2574903122.py:70: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
      "Epoch 2/20 - Training: 100%|██████████████████████| 5/5 [00:41<00:00,  8.30s/it]\n",
      "Epoch 2/20 - Validation: 100%|████████████████████| 5/5 [00:36<00:00,  7.20s/it]\n",
      "Epoch 2/20 - Testing: 100%|███████████████████████| 5/5 [00:35<00:00,  7.06s/it]\n",
      "Epoch 3/20 - Training: 100%|██████████████████████| 5/5 [00:38<00:00,  7.73s/it]\n",
      "Epoch 3/20 - Validation: 100%|████████████████████| 5/5 [00:35<00:00,  7.04s/it]\n",
      "Epoch 3/20 - Testing: 100%|███████████████████████| 5/5 [00:35<00:00,  7.09s/it]\n",
      "Epoch 4/20 - Training: 100%|██████████████████████| 5/5 [00:41<00:00,  8.28s/it]\n",
      "Epoch 4/20 - Validation: 100%|████████████████████| 5/5 [00:36<00:00,  7.32s/it]\n",
      "Epoch 4/20 - Testing: 100%|███████████████████████| 5/5 [00:36<00:00,  7.28s/it]\n",
      "Epoch 5/20 - Training: 100%|██████████████████████| 5/5 [00:40<00:00,  8.04s/it]\n",
      "Epoch 5/20 - Validation: 100%|████████████████████| 5/5 [00:36<00:00,  7.37s/it]\n",
      "Epoch 5/20 - Testing: 100%|███████████████████████| 5/5 [00:36<00:00,  7.26s/it]\n",
      "Epoch 6/20 - Training: 100%|██████████████████████| 5/5 [00:40<00:00,  8.04s/it]\n",
      "Epoch 6/20 - Validation: 100%|████████████████████| 5/5 [00:36<00:00,  7.35s/it]\n",
      "Epoch 6/20 - Testing: 100%|███████████████████████| 5/5 [00:36<00:00,  7.38s/it]\n",
      "Epoch 7/20 - Training: 100%|██████████████████████| 5/5 [00:40<00:00,  8.10s/it]\n",
      "Epoch 7/20 - Validation: 100%|████████████████████| 5/5 [00:35<00:00,  7.19s/it]\n",
      "Epoch 7/20 - Testing: 100%|███████████████████████| 5/5 [00:35<00:00,  7.13s/it]\n",
      "Epoch 8/20 - Training: 100%|██████████████████████| 5/5 [00:40<00:00,  8.17s/it]\n",
      "Epoch 8/20 - Validation: 100%|████████████████████| 5/5 [00:36<00:00,  7.30s/it]\n",
      "Epoch 8/20 - Testing: 100%|███████████████████████| 5/5 [00:35<00:00,  7.20s/it]\n",
      "Epoch 9/20 - Training: 100%|██████████████████████| 5/5 [00:41<00:00,  8.20s/it]\n",
      "Epoch 9/20 - Validation: 100%|████████████████████| 5/5 [00:37<00:00,  7.46s/it]\n",
      "Epoch 9/20 - Testing: 100%|███████████████████████| 5/5 [00:36<00:00,  7.25s/it]\n",
      "Epoch 10/20 - Training: 100%|█████████████████████| 5/5 [00:39<00:00,  7.92s/it]\n",
      "Epoch 10/20 - Validation: 100%|███████████████████| 5/5 [00:36<00:00,  7.32s/it]\n",
      "Epoch 10/20 - Testing: 100%|██████████████████████| 5/5 [00:36<00:00,  7.38s/it]\n",
      "Epoch 11/20 - Training: 100%|█████████████████████| 5/5 [00:40<00:00,  8.09s/it]\n",
      "Epoch 11/20 - Validation: 100%|███████████████████| 5/5 [00:36<00:00,  7.38s/it]\n",
      "Epoch 11/20 - Testing: 100%|██████████████████████| 5/5 [00:36<00:00,  7.27s/it]\n",
      "Epoch 12/20 - Training: 100%|█████████████████████| 5/5 [00:39<00:00,  7.93s/it]\n",
      "Epoch 12/20 - Validation: 100%|███████████████████| 5/5 [00:37<00:00,  7.56s/it]\n",
      "Epoch 12/20 - Testing: 100%|██████████████████████| 5/5 [00:37<00:00,  7.44s/it]\n",
      "Epoch 13/20 - Training: 100%|█████████████████████| 5/5 [00:41<00:00,  8.28s/it]\n",
      "Epoch 13/20 - Validation: 100%|███████████████████| 5/5 [00:36<00:00,  7.20s/it]\n",
      "Epoch 13/20 - Testing: 100%|██████████████████████| 5/5 [00:36<00:00,  7.22s/it]\n",
      "Epoch 14/20 - Training: 100%|█████████████████████| 5/5 [00:40<00:00,  8.08s/it]\n",
      "Epoch 14/20 - Validation: 100%|███████████████████| 5/5 [00:36<00:00,  7.25s/it]\n",
      "Epoch 14/20 - Testing: 100%|██████████████████████| 5/5 [00:35<00:00,  7.19s/it]\n",
      "Epoch 15/20 - Training: 100%|█████████████████████| 5/5 [00:42<00:00,  8.44s/it]\n",
      "Epoch 15/20 - Validation: 100%|███████████████████| 5/5 [00:39<00:00,  7.98s/it]\n",
      "Epoch 15/20 - Testing: 100%|██████████████████████| 5/5 [00:39<00:00,  7.86s/it]\n",
      "Epoch 16/20 - Training: 100%|█████████████████████| 5/5 [00:43<00:00,  8.66s/it]\n",
      "Epoch 16/20 - Validation: 100%|███████████████████| 5/5 [00:39<00:00,  7.90s/it]\n",
      "Epoch 16/20 - Testing: 100%|██████████████████████| 5/5 [00:38<00:00,  7.61s/it]\n",
      "Epoch 17/20 - Training: 100%|█████████████████████| 5/5 [00:42<00:00,  8.49s/it]\n",
      "Epoch 17/20 - Validation: 100%|███████████████████| 5/5 [00:39<00:00,  7.95s/it]\n",
      "Epoch 17/20 - Testing: 100%|██████████████████████| 5/5 [00:36<00:00,  7.34s/it]\n",
      "Epoch 18/20 - Training: 100%|█████████████████████| 5/5 [00:39<00:00,  7.85s/it]\n",
      "Epoch 18/20 - Validation: 100%|███████████████████| 5/5 [00:34<00:00,  6.96s/it]\n",
      "Epoch 18/20 - Testing: 100%|██████████████████████| 5/5 [00:35<00:00,  7.10s/it]\n",
      "Epoch 19/20 - Training: 100%|█████████████████████| 5/5 [00:42<00:00,  8.53s/it]\n",
      "Epoch 19/20 - Validation: 100%|███████████████████| 5/5 [00:37<00:00,  7.56s/it]\n",
      "Epoch 19/20 - Testing: 100%|██████████████████████| 5/5 [00:36<00:00,  7.27s/it]\n",
      "Epoch 20/20 - Training: 100%|█████████████████████| 5/5 [00:41<00:00,  8.21s/it]\n",
      "Epoch 20/20 - Validation: 100%|███████████████████| 5/5 [00:40<00:00,  8.00s/it]\n",
      "Epoch 20/20 - Testing: 100%|██████████████████████| 5/5 [00:36<00:00,  7.39s/it]\n"
     ]
    }
   ],
   "source": [
    "output_folder = \"/Users/ishaanbabbar/Desktop/GT Summer 2024/DSAN 6500/Project/wsirois/output_google\"\n",
    "\n",
    "# Load and modify the pretrained googlenet model\n",
    "model = models.googlenet(pretrained =True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 150)\n",
    "model.to(device)\n",
    "\n",
    "#num_classes = len(train_dataset.classes)\n",
    "#model.classifier = nn.Sequential(\n",
    "#    nn.Dropout(p=0.2, inplace=False),\n",
    "#    nn.Linear(model.classifier[1].in_features, num_classes)\n",
    "#)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Learning rate scheduler - define your warmup_cosine_annealing scheduler here\n",
    "\n",
    "num_epochs = 20\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 0.1**(epoch // 30))\n",
    "\n",
    "writer = SummaryWriter('../runs/googlenet_experiment')\n",
    "\n",
    "# Initialize DataFrame to store metrics\n",
    "columns = [\n",
    "    'Epoch', 'Training Loss', 'Validation Loss', 'Test Loss',\n",
    "    'Training Accuracy', 'Validation Accuracy', 'Test Accuracy',\n",
    "    'Training Precision', 'Training Recall', 'Training F1-Score',\n",
    "    'Validation Precision', 'Validation Recall', 'Validation F1-Score',\n",
    "    'Test Precision', 'Test Recall', 'Test F1-Score',\n",
    "    'Epoch Duration', 'Training Top-5 Accuracy', 'Validation Top-5 Accuracy', 'Test Top-5 Accuracy'\n",
    "]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "best_val_accuracy = 0  # Initialize best validation accuracy for checkpointing\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_accuracy, train_prec, train_rec, train_f1, train_top5_acc = train_one_epoch(\n",
    "    model, train_loader, criterion, optimizer, device, epoch, num_epochs)\n",
    "\n",
    "    val_loss, val_accuracy, val_prec, val_rec, val_f1, val_top5_acc = evaluate(\n",
    "    model, validation_loader, criterion, device, epoch, num_epochs, phase='Validation')\n",
    "\n",
    "    test_loss, test_accuracy, test_prec, test_rec, test_f1, test_top5_acc = evaluate(\n",
    "    model, test_loader, criterion, device, epoch, num_epochs, phase='Testing')\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    scheduler.step()\n",
    "\n",
    "    # Log to TensorBoard\n",
    "    writer.add_scalars('Loss', {'Train': train_loss, 'Validation': val_loss, 'Test': test_loss}, epoch)\n",
    "    writer.add_scalars('Accuracy', {'Train': train_accuracy, 'Validation': val_accuracy, 'Test': test_accuracy}, epoch)\n",
    "    writer.add_scalars('Precision', {'Train': train_prec, 'Validation': val_prec, 'Test': test_prec}, epoch)\n",
    "    writer.add_scalars('Recall', {'Train': train_rec, 'Validation': val_rec, 'Test': test_rec}, epoch)\n",
    "    writer.add_scalars('F1-Score', {'Train': train_f1, 'Validation': val_f1, 'Test': test_f1}, epoch)\n",
    "    writer.add_scalars('Top-5 Accuracy', {'Train': train_top5_acc, 'Validation': val_top5_acc, 'Test': test_top5_acc}, epoch)\n",
    "\n",
    "    # Update DataFrame\n",
    "    new_row = {\n",
    "        'Epoch': epoch + 1, 'Training Loss': train_loss, 'Validation Loss': val_loss, 'Test Loss': test_loss,\n",
    "        'Training Accuracy': train_accuracy, 'Validation Accuracy': val_accuracy, 'Test Accuracy': test_accuracy,\n",
    "        'Training Precision': train_prec, 'Training Recall': train_rec, 'Training F1-Score': train_f1,\n",
    "        'Validation Precision': val_prec, 'Validation Recall': val_rec, 'Validation F1-Score': val_f1,\n",
    "        'Test Precision': test_prec, 'Test Recall': test_rec, 'Test F1-Score': test_f1,\n",
    "        'Epoch Duration': epoch_duration, 'Training Top-5 Accuracy': train_top5_acc, 'Validation Top-5 Accuracy': val_top5_acc,\n",
    "        'Test Top-5 Accuracy': test_top5_acc\n",
    "    }\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    # Checkpointing\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), os.path.join(output_folder, f'googlenet_best.pth'))\n",
    "\n",
    "writer.close()\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "csv_filename = f\"googlenet_training_results_{timestamp}.csv\"\n",
    "df.to_csv(os.path.join(output_folder, csv_filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f7ee4-bbf3-485f-b588-c5caad061c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure 'Test Loss' and 'Validation Loss' are present in your DataFrame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot training, test, and validation loss\n",
    "plt.plot(df['Epoch'], df['Training Loss'], linestyle='-', color='blue', label='Training Loss')\n",
    "plt.plot(df['Epoch'], df['Test Loss'], linestyle='--', color='green', label='Test Loss')\n",
    "\n",
    "# Optionally, plot validation and test accuracy if you wish to visualize accuracy on a secondary axis\n",
    "plt.title('Training, Validation, and Test Loss per Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(range(1, num_epochs + 1, 2))  # Adjust this based on your actual number of epochs\n",
    "plt.grid(True, which='both', axis='both', linestyle='--', color=\"#eaeaea\")\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Create a secondary y-axis to plot accuracy if needed\n",
    "sec_axis = plt.twinx()\n",
    "sec_axis.set_ylabel('Accuracy')\n",
    "sec_axis.legend(loc='upper right')\n",
    "\n",
    "plt.savefig(os.path.join(output_folder, f\"training_validation_test_loss_plot_{modelname}_{timestamp}.png\"), dpi=300)  # Save the plot as a PNG file\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
